---
layout: post
title: "1조 기본 발제 자료"
date: 2024-08-06 03:01:45 +0900
categories: 발제-자료
---
## <span style= 'background-color: #f1f8ff'>3-1: k-최근접 이웃 회귀
☆ **회귀**: 지도 학습 알고리즘의 한 종류로 임의의 어떤 숫자를 예측하는 알고리즘. 정해진 클래스가 없으며 임의의 수치를 출력함.

☆ **k-최근접 이웃 회귀**: 예측하려는 샘플에 가장 가까운 샘플 k개를 선택한 후 해당 샘플들의 평균을 정답으로 사용하는 것. 이때 이웃한 샘플의 타깃은 어떤 클래스가 아니라 임의의 수치임.

☆ **k-최근접 이웃 회귀를 활용한 생선 무게 에측 프로그램**

➀ 데이터 준비: 데이터 리스트 준비, 산점도 표시, 훈련 세트와 테스트 세트 준비를 통해 프로그램에 사용될 전반적인 데이터 준비.
```python
import numpy as np
perch_length = np.array(
    [8.4, 13.7, 15.0, ~~~~~~, 44.0]
    )
perch_weight = np.array(
    [5.9, 32.0, 40.0, ~~~~~~, 1000.0]
    )

import matplotlib.pyplot as plt
plt.scatter(perch_length, perch_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42)

train_input = train_input.reshape(-1,1)
test_input = test_input.reshape(-1,1)
```
🚨 주의: 사이킷런에 사용할 훈련 세트는 2차원 배열이어야 함. 따라서 reshape()를 통해 크기 변경.

➁ 머신러닝 프로그램 훈련: 훈련 세트로 훈련 후 테스트 세트로 결정계수를 평가.
```python
from sklearn.neighbors import KNeighborsRegressor
knr = KNeighborsRegressor()
knr.fit(train_input, train_target)
print(knr.score(test_input, test_target))
```
➡︎ 0.9928094061010639

📌 결정계수(R^2): 회귀를 평가하는 점수. 1 - (타깃 - 예측)^2/(타깃 - 평균)^2의 식으로 구함.

⊕ 결정계수의 정도 파악을 위한 절댓값 오차 계산
```python
from sklearn.metrics import mean_absolute_error
test_prediction = knr.predict(test_input)
mae = mean_absolute_error(test_target, test_prediction)
print(mae)
```
➡︎ 19.157142857142862

➂ 과대적합•과소적합 해결: 훈련 세트와 테스트 세트의 결정계수 점수를 비교했을 때 훈련 세트가 너무 높은 **과대적합**, 그 반대이거나 두 점수가 모두 낮은 **과소적합**을 해결.
- 과대적합의 해결: 모델을 덜 복잡하게 만들어 훈련 세트에 대한 의존도를 줄임. k-최근접 이웃의 경우 k값을 늘려서 해결.
- 과소적합의 해결: 모델을 더 복잡하게 만들어 훈련 세트에 더 잘 맞게 만듦. k-최근접 이웃의 경우 k값을 줄여서 해결.

```python
print(knr.score(train_input, train_target))
```
➡︎ 0.9698823289099255 (과소적합)

```python
knr.n_neighbors = 3 
knr.fit(train_input, train_target)
print(knr.score(train_input))
```
➡︎ 0.9804899950518966

```python
print(knr.score(test_input, test_target))
```
➡︎ 0.974645996398761

## <span style= 'background-color: #f1f8ff'>3-2: 선형 회귀
☆ **k-최근접 이웃 회귀의 한계**: 가장 가까운 샘플을 찾아 타깃을 평균하기에 새로운 샘플이 훈련 세트의 범위를 벗어나면 엉뚱한 값을 예측할 수 있음.
![image](https://github.com/user-attachments/assets/a2edd1ec-d1ac-46a9-a87c-6a8bd6f913cb)
⬆︎ k-최근접 이웃 회귀의 한계

☆ **선형 회귀**: 특성과 타깃 사이의 관계를 가장 잘 나타내는 선형 방정식을 찾아 결과를 예측함.
![image](https://github.com/user-attachments/assets/fd1b7d16-8387-4fea-852f-934b33dad8cf)
⬆︎ 선형 회귀를 할 경우 위 그래프의 3과 같은 직선을 머신러닝 알고리즘이 자동으로 찾음.

```python
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(train_input, train_target)
print(lr.predict([[50]]))
```
➡︎ [1241.83860323]

```python
print(lr.coef_, lr.intercept_)
```
➡︎ [39.01714496] -709.0186449535477

```python
plt.scatter(train_input, train_target)
plt.plot([15, 50], [15*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])
plt.scatter(50, 1241.8, marker='^')
plt.show()
```
➡︎
![image](https://github.com/user-attachments/assets/7fe62594-0241-4880-9fd9-dbd68a1bed21)

📌 사이킷런 모델 클래스의 메서드명: 사이킷런의 모델 클래스들은 훈련, 평가, 예측하는 메서드 이름이 모두 동일.

📌 모델 파라미터: 머신러닝 알고리즘이 찾은 값. 즉 머신러닝 알고리즘의 훈련 과정은 최적의 모델 파라미터를 찾는 과정과 같음.

📌 사례 기반 학습: 모델 파라미터 없이 훈련 세트를 저장하는 것이 훈련의 전부인 학습. (ex: k-최근접 이웃)

```python
print(lr.score(train_input, train_target))
print(lr.score(test_input, test_target))
```
➡︎ 0.9398463339976039   
➡︎ 0.8247503123313558 (과소적합)

☆ **선형 회귀의 한계**: 샘플들이 정확한 직선 분포가 아니라면 과소적합이 일어날 가능성이 큼. 아래 그래프에서 농어의 무게가 음수 값을 가질 것이라 예상하는 것처럼 현실에서 불가능한 예측이 일어날 가능성이 있음.

☆ **다항 회귀**: 다항식을 사용한 선형 회귀.
```python
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))

lr = LinearRegression()
lr.fit(train_poly, train_target)

print(lr.coef_, lr.intercept_) 
```
➡︎ [ 1.01433211 -21.55792498] 116.0502107827827

```python
point = np.arrange(15, 50)
plt.scatter(train_input, train_target)
plt.plot(point, 1.01*point**2 - 21.6*point + 116.05)
plt.scatter([50], [1574], marker='^')
plt.show()
```
➡︎
![image](https://github.com/user-attachments/assets/e065574d-db5e-4afe-a2cf-31efe7a404e8)

```python
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))
```
➡︎ 0.9706807451768623
➡︎ 0.9775935108325122 (과소적합)

☆ **다항 회귀의 한계**: 훈련 세트와 테스트 세트에 대한 점수가 선형 회귀보다는 크게 높아졌지만 여전히 과소적합이 남아있음.

## <span style= 'background-color: #f1f8ff'>3-3: 특성 공학과 규제
☆ **다중 회귀**: 여러 개의 특성을 사용한 선형 회귀. 특성이 2개일 경우 선형 회귀는 평면을 학습함. 특성이 많은 고차원으로 갈 수록 선형 회귀는 매우 복잡한 모델을 표현할 수 있음.
![image](https://github.com/user-attachments/assets/8c23db29-2bca-45c7-8b6e-d47b1d69524d)
⬆︎ 특성이 2개일 때와 3개일 때의 선형 회귀 모형

☆ **특성 공학**: 기존의 특성을 사용해 새로운 특성을 뽑아내는 작업. 예를 들어 이전 예제에서의 농어 길이를 제곱할 수 있고, 농어 길이와 농어 높이를 곱할 수도 있음.

☆ **다중 회귀를 활용한 생선 무게 예측 프로그램**

➀ 데이터 준비: 판다스를 활용하여 더 간단하게 데이터 준비.
```python
import pandas as pd
df = pd.read_csv('htttps://bit.ly/perch_csv')
perch_full = df.to_numpy()
print(perch_full)
```
➡︎

![image](https://github.com/user-attachments/assets/19797d1c-7420-4787-a8e4-1092ecd6f44a)

📌 CSV 파일: 판다스 데이터프레임을 만들기 위해 많이 사용하는 파일로 콤마로 나누어져 있는 텍스트 파일.

```python
import numpy as np
perch_weight = np.array(
    [5.9, 32.0, 40.0, ~~~~~, 1000.0]
    )
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42)
```

➁ 변환기를 활용한 특성 준비: 사이킷런에서 특성을 만들거나 전처리하기 위해 제공하는 클래스인 변환기를 활용하여 다양한 특성들을 준비.
```python
from sklearn.preprocessing import PlolynomialFeatures
poly = PolynomialFeatures()
poly = PolynomialFeatures(include_bias=False) 
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
print(train_poly.shape)
```
➡︎ (42, 9) (9개의 특성)

🚨 주의: transform 전에는 꼭 fit을 사용해야함. 사이킷런의 일관된 api 때문에 두 단계로 나뉘어져 있음.

📌 transform에서 1을 출력하는 이유: 선형 방정식의 절편은 1에 곱해지는 계수라고 볼 수 있음. 사이킷런의 선형 모델은 자동으로 절편을 추가하기에 1을 없애도 됨.

⊕ 9개의 특성의 입력 조합 확인
```python
poly.get_feature_names()
```
➡︎ ['x0', x'1,' 'x2', "x0^2' ',X"O x1', X"O x2", "x1^2", "x1 x2', "x2^2"]

➂ 다중 회귀 모델 훈련: 여러 개의 특성을 사용하여 선형 회귀를 수행하는 것과 같음.
```python
from sklearn.linear_model import LinearRegression
lr = LinearRegression
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))
```
➡︎ 0.9903183436982124   
➡︎ 0.9714559911594134 (과소적합 해결)

🚨 주의: PolynomialFeatures 클래스의 degree 매개변수를 사용하면 필요한 고차항의 최대 차수를 지정할 수 있음. **다만** 너무 고차원이 될 경우 훈련 세트에 너무 과대적합되므로 테스트 세트에서는 형편없는 점수를 만들 수 있음.

☆ **규제**: 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 하는 것. 선형 회귀 모델의 경우 특성에 곱해지는 계수의 크기를 작게 만들어 행할 수 있음.
![image](https://github.com/user-attachments/assets/2da1ed9a-2bbe-47ee-b294-d78cad23fe88)
⬆︎ 선형 회귀 규제의 원리

📌 규제의 장점: 많은 특성을 사용했음에도 불구하고 훈련 세트에 너무 과대적합되지 않아 테스트 세트에서도 좋은 성능을 낼 수 있음.

🚨 주의: 특성의 스케일이 정규화되지 않으면 계수 값에 차이가 나게 되기에 규제를 적용하기 전에 먼저 정규화를 해야함.

⊕StandardScaler를 활용한 정규화
```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_poly) 
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```

☆ **릿지 회귀**: 계수를 제곱한 값을 기준으로 규제를 적용한 선형 회귀 모델.
```python
from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))
```
➡︎ 0.9896101671037343   
➡︎ 0.9790693977615398

⊕ alpha를 활용한 규제의 강도 조절(alpha가 커지며 규제 강도가 세짐)
```python
import matplotlin.pyplot as plt
train_score = [] #alpha 값을 바꿀 때마다 score() 메서드의 결과를 저장할 리스트를 생성하는 명령어
test_score = []
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] #사용할 alpha의 리스트를 지정하는 명령어
for alpha in alpha_list:
    ridge = Ridge(alpha=alpha) #릿지 모델을 만드는 명령어
    ridge.fit(train_scaled, train_target)
    train_score.append(ridege.score(train_scaled, train_target)) #훈련 점수를 저장
    train_score.append(ridge.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.show()
```
➡︎
![image](https://github.com/user-attachments/assets/aba768c9-5207-4e3a-a298-6674ea2ae7b3)
⬆︎ alpha 값에 따른 훈련 세트와 테스트 세트의 점수를 나타낸 그래프. 두 그래프가 가장 가깝고 테스트 세트의 점수가 가장 높은 -1, 즉 0.1이 가장 적절한 alpha 값.

📌 그래프를 로그 스케일로 그리는 이유: 0.001부터 10배씩 늘렸기 때문에 그대로 그래프를 그리면 왼쪽이 너무 촘촘해짐.

```python
ridge = Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))
```
➡︎ 0.9903815817570366   
➡︎ 0.9827976465386926

☆ **라쏘 회귀**: 계수의 절댓값을 기준으로 규제를 적용한 선형 회귀 모델. 계수를 0으로 만들 수도 있음.
```python
from sklearn.linear_model import Lasso
lasso = Lasso()
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
```
➡︎ 0.9897898972080961   
➡︎ 0.9800593698421883

⊕ alpha를 활용한 규제의 강도 조절(alpha가 커지며 규제 강도가 세짐)
```python
train_score = [] #alpha 값을 바꿀 때마다 score() 메서드의 결과를 저장할 리스트를 생성하는 명령어
test_score = []
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] #사용할 alpha의 리스트를 지정하는 명령어
for alpha in alpha_list:
    lasso = Lasso(alpha=alpha, max_iter=10000) #라쏘 모델을 만드는 명령어. max_iter는 반복 횟수를 늘리기 위한 명령어.
    lasso.fit(train_scaled, train_target)
    train_score.append(lasso.score(train_scaled, train_target)) #훈련 점수를 저장
    train_score.append(lasso.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.show()
```
➡︎
![image](https://github.com/user-attachments/assets/4f09adf0-29f5-486c-8b8a-82b43b0fee68)
⬆︎ alpha 값에 따른 훈련 세트와 테스트 세트의 점수를 나타낸 그래프. 두 그래프가 가장 가깝고 테스트 세트의 점수가 가장 높은 1, 즉 10이 가장 적절한 alpha 값.

```python
lasso = Lasso(alpha=10)
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
```
➡︎ 0.9888067471131867   
➡︎ 0.9824470598706695

### 출처 : 혼자 공부하는 머신러닝+딥러닝